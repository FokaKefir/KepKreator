{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FokaKefir/KepKreator/blob/main/MNIST_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when running in colab\n",
    "#!pip install wandb\n",
    "#!pip install keras_tuner\n",
    "#!git clone https://github.com/FokaKefir/KepKreator.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/content/KepKreator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOixpLNLNLj5"
   },
   "source": [
    "# Import dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7I59B7OvNDTP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Conv2DTranspose, Input, Flatten, Reshape, Embedding, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import load_model, save_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow import keras\n",
    "import keras_tuner\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "import sys, os\n",
    "\n",
    "VAL_MODEL_PATH = 'validation_cnn.hdf5'\n",
    "PROJECT = 'KepKreator'\n",
    "WANDB_IMAGE_NUM = 8\n",
    "VAL_METRIC_N_SAMPLE = 1024\n",
    "USE_WANDB = True\n",
    "N_CLASSES = 10\n",
    "\n",
    "# disable warnings\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvAsA8tPhe_B"
   },
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BJrSWPQSG4h"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaYBacb7NKv9",
    "outputId": "bd22962b-ea0c-4bd1-c3fd-284f6cb3636c"
   },
   "outputs": [],
   "source": [
    "(x_train, labels_train), (x_test, labels_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0 * 2 - 1, x_test / 255.0 * 2 - 1\n",
    "x_train = np.concatenate([x_train, x_test])\n",
    "labels_train = np.concatenate([labels_train, labels_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekU665vsSN0S"
   },
   "source": [
    "## Plot some random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hsozFwtXOaW4",
    "outputId": "686a5897-89b8-44ec-fd7b-12cc0f6bb351"
   },
   "outputs": [],
   "source": [
    "def show_images(examples):\n",
    "  random_indices = np.random.choice(examples.shape[0], 9, replace=False)\n",
    "\n",
    "  # rescale images (-1, +1) -> (0, 1)\n",
    "  examples = 0.5 * examples + 0.5\n",
    "\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  for i, index in enumerate(random_indices, 1):\n",
    "      plt.subplot(3, 3, i)\n",
    "      plt.imshow(x_train[index], cmap='gray')\n",
    "      plt.title(f\"Label: {labels_train[index]}\")\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for the generated images\n",
    "\n",
    "Possible metrics:\n",
    "\n",
    "* inception score \n",
    "    * [medium article](https://medium.com/octavian-ai/a-simple-explanation-of-the-inception-score-372dff6a8c7a)\n",
    "    * [short code introduction](https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/)\n",
    "* parzen window estimation\n",
    "* conditional inception scores\n",
    "    * [Evaluation Metrics for Conditional Image Generation](https://arxiv.org/abs/2004.12361)\n",
    "* Frechet inception distance\n",
    "* kernel inception distance\n",
    "    * [KID code and GAN tips and trics](https://keras.io/examples/generative/gan_ada/#dataefficient-gans-with-adaptive-discriminator-augmentation)\n",
    "\n",
    "## Conditional inception scores and inception score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(a, b):\n",
    "   '''Kullback-Leibler divergence\n",
    "   Smaller value means more similar distributions'''\n",
    "   a, b = np.atleast_2d(np.asarray(a)), np.atleast_2d(np.asarray(b))\n",
    "   eps = 1e-16\n",
    "   Dkls = a * np.log((a + eps) / (b + eps))\n",
    "   return Dkls.sum(axis=1)\n",
    "\n",
    "class InceptionScore(keras.metrics.Metric):\n",
    "    '''Inception score\n",
    "    IS = exp( Ex{ Dkl( pG_yx || pG_y ) } )'''\n",
    "\n",
    "    def __init__(self, name='inception_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mean_tracker = keras.metrics.Mean()\n",
    "\n",
    "    def update_state(self, y_hat):\n",
    "        IS = self._inception_score(y_hat)\n",
    "        self.mean_tracker.update_state(IS)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mean_tracker.result()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.mean_tracker.reset_state()\n",
    "\n",
    "    def _inception_score(self, y_hat):\n",
    "        p_y = np.expand_dims(np.mean(y_hat, axis=0), 0)\n",
    "        # kl divergence for each image\n",
    "        Dkls = kl_divergence(y_hat, p_y)\n",
    "        return np.exp(Dkls.mean())\n",
    "\n",
    "class WithinClassInceptionScore(keras.metrics.Metric):\n",
    "    '''Within-class inception score\n",
    "    Measures the quality and diversity for each of the classes.\n",
    "    High WCIS indicates a wide coverage of real classes within the \n",
    "    conditioned classes, which is an undesired property.\n",
    "    wcis = exp( Ec{ Dkl( pG_yx || pG_yc ) } )'''\n",
    "\n",
    "    def __init__(self, name='within_class_inception_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mean_tracker = keras.metrics.Mean()\n",
    "\n",
    "    def update_state(self, y_hat, gen_labels):\n",
    "        wcis = self._wcis(y_hat, gen_labels)\n",
    "        self.mean_tracker.update_state(wcis)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mean_tracker.result()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.mean_tracker.reset_state()\n",
    "\n",
    "    def _wcis(self, y_hat, labels):\n",
    "        classes = np.unique(labels)\n",
    "        Dkl = np.ndarray(len(classes))\n",
    "\n",
    "        I = np.ndarray(len(classes))\n",
    "        for i, cond in enumerate(classes):\n",
    "            # mean of pG_yx with the given condition, as in BCIS\n",
    "            pG_yc = np.expand_dims(np.mean(y_hat[labels == cond], axis=0), 0)\n",
    "            # x is sampled from the given class\n",
    "            pG_yx = y_hat[labels == cond]\n",
    "            # kl divergence for each sample within the class\n",
    "            Dkl = kl_divergence(pG_yx, pG_yc)\n",
    "            I[i] = Dkl.mean(axis=0)\n",
    "        return np.exp(I.mean())\n",
    "    \n",
    "class BetweenClassInceptionScore(keras.metrics.Metric):\n",
    "    '''Between-class inception score\n",
    "    Measures how close the representation of classes is to real data.\n",
    "    Higher BCIS is better, it indicates the distinct class representation\n",
    "    of the conditioned classes and a wide coverage across the conditional\n",
    "    classes, which is desired.\n",
    "    bcis = exp( Ec{ Dkl( pG_yc || pG_y ) } )'''\n",
    "\n",
    "    def __init__(self, name='between_class_inception_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mean_tracker = keras.metrics.Mean()\n",
    "\n",
    "    def update_state(self, y_hat, gen_labels):\n",
    "        bcis = self._bcis(y_hat, gen_labels)\n",
    "        self.mean_tracker.update_state(bcis)\n",
    "\n",
    "    def result(self):\n",
    "        return self.mean_tracker.result()\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.mean_tracker.reset_state()\n",
    "\n",
    "    def _bcis(self, y_hat, labels):\n",
    "        pG_y = np.expand_dims(np.mean(y_hat, axis=0), 0) # mean for all generated samples of pG_yx\n",
    "        \n",
    "        classes = np.unique(labels)\n",
    "        Dkl = np.ndarray(len(classes))\n",
    "        for i, cond in enumerate(classes):\n",
    "            # mean on classes of the pG_yx values\n",
    "            pG_yc = np.expand_dims(np.mean(y_hat[labels == cond], axis=0), 0)\n",
    "            Dkl[i] = kl_divergence(pG_yc, pG_y)[0]\n",
    "        return np.exp(Dkl.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rK5webqllwq5"
   },
   "source": [
    "# Define Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMKJKDuySO7H"
   },
   "outputs": [],
   "source": [
    "class CGAN(tf.keras.Model):\n",
    "  def __init__(self, config):\n",
    "    super(CGAN, self).__init__()\n",
    "\n",
    "    self.latent_dim = config['latent_dim']\n",
    "    self.n_classes = config['n_classes']\n",
    "\n",
    "    # create generator\n",
    "    self.generator = self.build_generator(\n",
    "      latent_dim=self.latent_dim, n_classes=self.n_classes, label_embedding=config['gen_label_embedding'],\n",
    "      label_hidden=config['gen_label_hidden'], input_hidden=config['gen_input_hidden'],\n",
    "      conv1_channels=config['gen_conv1_channels'], conv2_channels=config['gen_conv2_channels'])\n",
    "\n",
    "    # create discriminator\n",
    "    self.discriminator = self.build_discriminator(\n",
    "      n_classes=self.n_classes, label_embedding=config['disc_label_embedding'],\n",
    "      conv1_channels=config['disc_conv1_channels'], conv2_channels=config['disc_conv2_channels'],\n",
    "      dense_hidden=config['disc_dense_hidden'])\n",
    "\n",
    "    # classificator model used for validation\n",
    "    self.val_model = load_model(VAL_MODEL_PATH)\n",
    "\n",
    "    # add tracker\n",
    "    self.gen_loss_tracker = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
    "    self.disc_loss_tracker = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
    "    self.real_accuracy_tracker = keras.metrics.BinaryAccuracy(name=\"real_accuracy\")\n",
    "    self.fake_accuracy_tracker = keras.metrics.BinaryAccuracy(name=\"fake_accuracy\")\n",
    "    self.is_tracker = InceptionScore()\n",
    "    self.wcis_tracker = WithinClassInceptionScore()\n",
    "    self.bcis_tracker = BetweenClassInceptionScore()\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return [self.gen_loss_tracker, \n",
    "            self.disc_loss_tracker,\n",
    "            self.real_accuracy_tracker,\n",
    "            self.fake_accuracy_tracker,\n",
    "            self.is_tracker,\n",
    "            self.wcis_tracker,\n",
    "            self.bcis_tracker,]\n",
    "  \n",
    "  @property\n",
    "  def train_metrics(self):\n",
    "    return [self.gen_loss_tracker,\n",
    "            self.disc_loss_tracker,\n",
    "            self.real_accuracy_tracker,\n",
    "            self.fake_accuracy_tracker,]\n",
    "  \n",
    "  @property\n",
    "  def val_metrics(self):\n",
    "    return [self.is_tracker,\n",
    "            self.wcis_tracker,\n",
    "            self.bcis_tracker,]\n",
    "\n",
    "  def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "\n",
    "    # set optimizers\n",
    "    self.d_optimizer = d_optimizer\n",
    "    self.g_optimizer = g_optimizer\n",
    "\n",
    "    # set loss function\n",
    "    self.loss_fn = loss_fn\n",
    "\n",
    "    super(CGAN, self).compile()\n",
    "\n",
    "  def generate_latent_points(self, n_samples):\n",
    "    x_input = np.random.randn(self.latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, self.latent_dim)\n",
    "    x_labels = np.random.randint(0, self.n_classes, n_samples)\n",
    "    return [x_input, x_labels]\n",
    "\n",
    "  def generate_fake_samples(self, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input, x_labels = self.generate_latent_points(n_samples)\n",
    "\n",
    "    # predict outputs\n",
    "    imgs = self.generator([x_input, x_labels], training=False)\n",
    "\n",
    "    # create class labels\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return [imgs, x_labels], y\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "    # Unpack the data\n",
    "    real_images, real_labels = data\n",
    "\n",
    "    # Get batch size\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "    # Get randomly selected 'real' samples, with labels\n",
    "    x_real, y_real = [real_images, real_labels], np.ones((batch_size, 1))\n",
    "\n",
    "    # Update discriminator model weights\n",
    "    with tf.GradientTape() as tape:\n",
    "      real_predictions = self.discriminator(x_real)\n",
    "      d_loss_real = self.loss_fn(y_real, real_predictions)\n",
    "    grads = tape.gradient(d_loss_real, self.discriminator.trainable_weights)\n",
    "    self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "    # Generate 'fake' examples, with labels\n",
    "    x_fake, y_fake = self.generate_fake_samples(batch_size)\n",
    "\n",
    "    # Update discriminator model weights\n",
    "    with tf.GradientTape() as tape:\n",
    "      fake_predictions = self.discriminator(x_fake)\n",
    "      d_loss_fake = self.loss_fn(y_fake, fake_predictions)\n",
    "    grads = tape.gradient(d_loss_fake, self.discriminator.trainable_weights)\n",
    "    self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "    # Calculate loss\n",
    "    d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "    # Prepare points in latent space as input for the generator\n",
    "    x_gan = self.generate_latent_points(2 * batch_size)\n",
    "\n",
    "    # Create inverted labels for the fake samples\n",
    "    y_gan = np.ones((2 * batch_size, 1))\n",
    "\n",
    "    # Update the generator via the discriminator's error\n",
    "    with tf.GradientTape() as tape:\n",
    "      fake_imgs = self.generator(x_gan)\n",
    "      fake_imgs_and_labels = [fake_imgs, x_gan[1]]\n",
    "      predictions = self.discriminator(fake_imgs_and_labels)\n",
    "      g_loss = self.loss_fn(y_gan, predictions)\n",
    "    grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "    self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "    # Monitor loss\n",
    "    self.gen_loss_tracker.update_state(g_loss)\n",
    "    self.disc_loss_tracker.update_state(d_loss)\n",
    "    self.fake_accuracy_tracker.update_state(0.0, fake_predictions)\n",
    "    self.real_accuracy_tracker.update_state(1.0, real_predictions)\n",
    "    return {metric.name: metric.result() for metric in self.train_metrics}\n",
    "  \n",
    "  def build_discriminator(self, in_shape=(28, 28, 1), n_classes=10, label_embedding=50,\n",
    "                          conv1_channels=32, conv2_channels=64, dense_hidden=128, use_batchnorm=False):\n",
    "    # label input\n",
    "    i_label = Input(shape=(1, ))\n",
    "    x_label = Embedding(n_classes, label_embedding)(i_label)\n",
    "    x_label = Dense(in_shape[0] * in_shape[1], activation='tanh')(x_label)\n",
    "    x_label = Reshape((in_shape[0], in_shape[1], 1))(x_label)\n",
    "\n",
    "    # image input\n",
    "    i_img = Input(shape=in_shape)\n",
    "\n",
    "    # concatenate\n",
    "    x = Concatenate()([x_label, i_img])\n",
    "\n",
    "    # conv layers\n",
    "    x = Conv2D(conv1_channels, (3, 3), strides=(2, 2), padding='same', activation='tanh')(i_img)\n",
    "    x = Conv2D(conv2_channels, (3, 3), strides=(2, 2), padding='same', activation='tanh')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(dense_hidden, activation='tanh')(x)\n",
    "    if use_batchnorm:\n",
    "      x = keras.layers.BatchNormalization()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model([i_img, i_label], x)\n",
    "    return model\n",
    "\n",
    "  def build_generator(self, latent_dim=50, n_classes=10, label_embedding=50,\n",
    "                      label_hidden=16, input_hidden=16,\n",
    "                      conv1_channels=128, conv2_channels=128):\n",
    "    # label input\n",
    "    i_label = Input(shape=(1, ))\n",
    "    x_label = Embedding(n_classes, label_embedding)(i_label)\n",
    "    x_label = Dense(7 * 7 * label_hidden, activation='tanh')(x_label)\n",
    "    x_label = Reshape((7, 7, label_hidden))(x_label)\n",
    "\n",
    "    # foundation for 7x7 image\n",
    "    i_lat = Input(shape=(latent_dim, ))\n",
    "    x_lat = Dense(7 * 7 * input_hidden, activation='tanh')(i_lat)\n",
    "    x_lat = Reshape((7, 7, input_hidden))(x_lat)\n",
    "\n",
    "    # concatenate\n",
    "    x = Concatenate()([x_lat, x_label])\n",
    "\n",
    "    # upsample to 14x14\n",
    "    x = Conv2DTranspose(conv1_channels, (4, 4), strides=(2, 2), padding='same', activation='tanh')(x)\n",
    "\n",
    "    # upsample to 28x28\n",
    "    x = Conv2DTranspose(conv2_channels, (4, 4), strides=(2, 2), padding='same', activation='tanh')(x)\n",
    "\n",
    "    # make only one color channel, values in (-1, +1)\n",
    "    x = Conv2D(1, (7, 7), activation='tanh', padding='same')(x)\n",
    "    return Model([i_lat, i_label], x)\n",
    "\n",
    "  def sample_images(self, e=0, b=0, method='show'):\n",
    "    if not os.path.exists('gan_images'):\n",
    "      os.makedirs('gan_images')\n",
    "\n",
    "    data, _ = self.generate_fake_samples(25)\n",
    "    imgs, labels = data\n",
    "    rows, cols = 5, 5\n",
    "\n",
    "    # Rescale images (-1, +1) -> (0, 1)\n",
    "    imgs = 0.5 * imgs + 0.5\n",
    "\n",
    "    if method == 'show' or method == 'save':\n",
    "      fig, axs = plt.subplots(rows, cols, figsize=(9, 10))\n",
    "      idx = 0\n",
    "      for i in range(rows):\n",
    "        for j in range(cols):\n",
    "          axs[i, j].imshow(imgs[idx], cmap='gray')\n",
    "          axs[i, j].set_title(f'num: {labels[idx]}')\n",
    "          axs[i, j].axis('off')\n",
    "          idx += 1\n",
    "\n",
    "    if method == 'show':\n",
    "      plt.show()\n",
    "    elif method == 'save':\n",
    "      fig.savefig(f'gan_images/sample_{e:003d}_{b:0004d}.png')\n",
    "      plt.close()\n",
    "    elif method == 'wandb':\n",
    "      columns = ['epoch', 'batch', 'label_num', 'image']\n",
    "      im_table = wandb.Table(columns=columns)\n",
    "      for row in range(WANDB_IMAGE_NUM):\n",
    "        im_table.add_data(e, b, labels[row], wandb.Image(imgs[row]))\n",
    "      wandb.log({'generated_examples': im_table})\n",
    "\n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "    gen_images_and_labels, _ = self.generate_fake_samples(VAL_METRIC_N_SAMPLE)\n",
    "\n",
    "    gen_images, gen_labels = gen_images_and_labels[0], gen_images_and_labels[1]\n",
    "    y_hat = self.val_model(gen_images)\n",
    "\n",
    "    self.is_tracker.update_state(y_hat)\n",
    "    self.bcis_tracker.update_state(y_hat, gen_labels)\n",
    "    self.wcis_tracker.update_state(y_hat, gen_labels)\n",
    "\n",
    "    return {metric.name: metric.result() for metric in self.val_metrics}\n",
    "\n",
    "  # save models\n",
    "  def save(self, filepath, overwrite=True, save_format=None, **kwargs):\n",
    "    self.generator.save(filepath + 'generator.h5')\n",
    "    self.discriminator.save(filepath + 'discriminator.h5')\n",
    "\n",
    "  # load models\n",
    "  def load_models(self, generator_path, discriminator_path):\n",
    "    self.generator = load_model(generator_path, compile=False)\n",
    "    self.discriminator = load_model(discriminator_path, compile=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ1kMQVxiDtv"
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1hkIEkevKYg"
   },
   "source": [
    "## Define callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRWbBqyldK81"
   },
   "outputs": [],
   "source": [
    "class SamplerCallback(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, sample_freq: int=1):\n",
    "    super().__init__()\n",
    "    self.sample_freq = sample_freq\n",
    "    self.e = 0\n",
    "\n",
    "  def on_epoch_begin(self, epoch, logs=None):\n",
    "    self.e = epoch\n",
    "\n",
    "  def on_train_batch_end(self, batch: int, logs=None):\n",
    "    if batch % self.sample_freq == 0:\n",
    "      method = 'wandb' if USE_WANDB else 'save'\n",
    "      self.model.sample_images(self.e + 1, batch, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8OMtZrH3tNT"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class GarbageCollectorCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypermodel for hiperparameter optimization\n",
    "\n",
    "Sources:\n",
    "\n",
    "* [keras documentation for hyperparameter tuning with custom training loop](https://keras.io/guides/keras_tuner/custom_tuner/)\n",
    "* [kaggle notebook baout using keras-tuner with wandb](https://www.kaggle.com/code/aritrag/keras-tuner-with-wandb/notebook)\n",
    "* [keras documentation for keras-tuner Tuners](https://keras.io/guides/keras_tuner/getting_started/#tune-endtoend-workflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGANHypermodel(keras_tuner.HyperModel):\n",
    "    def build(self, hp):\n",
    "        config = {\n",
    "            'latent_dim': hp.Int('latent_dim', min_value=32, max_value = 128, step = 32),\n",
    "            'gen_label_embedding': hp.Choice('gen_label_embedding', [10, 25, 50]),\n",
    "            'gen_label_hidden': hp.Choice('gen_label_hidden', [1, 4, 8, 16, 32]),\n",
    "            'gen_input_hidden': hp.Choice('gen_input_hidden', [32, 64, 96]),\n",
    "            'gen_conv1_channels': hp.Choice('gen_conv1_channels', [16, 64, 128]),\n",
    "            'gen_conv2_channels': hp.Choice('gen_conv2_channels', [64, 128, 256]),\n",
    "            'disc_label_embedding': hp.Choice('disc_label_embedding', [10, 25, 50]),\n",
    "            'disc_conv1_channels': hp.Choice('disc_conv1_channels', [16, 64, 128]),\n",
    "            'disc_conv2_channels': hp.Choice('disc_conv2_channels', [64, 128, 256]),\n",
    "            'disc_dense_hidden': hp.Choice('disc_dense_hidden', [32, 64, 128]),\n",
    "            'disc_use_batchnorm': hp.Boolean('disc_use_batchnorm'),\n",
    "            'n_classes': hp.Fixed('n_classes', N_CLASSES),\n",
    "            'batch_size': hp.Fixed('batch_size', 32),\n",
    "            'n_val': hp.Fixed('n_val', VAL_METRIC_N_SAMPLE),\n",
    "            'VAL_MODEL_PATH': hp.Fixed('VAL_MODEL_PATH', VAL_MODEL_PATH),\n",
    "            'gen_learning_rate': hp.Fixed('gen_learning_rate', 0.0002),\n",
    "            'gen_adam_beta1': hp.Fixed('gen_adam_beta1', 0.5),\n",
    "            'discr_learning_rate': hp.Fixed('discr_learning_rate', 0.0002),\n",
    "            'discr_adam_beta1': hp.Fixed('discr_adam_beta1', 0.5),\n",
    "        }\n",
    "        self.last_build_config_keys = config.keys()\n",
    "        cgan = CGAN(config)\n",
    "        cgan.compile(\n",
    "            d_optimizer=Adam(learning_rate=config['discr_learning_rate'],\n",
    "                            beta_1=config['discr_adam_beta1']),\n",
    "            g_optimizer=Adam(learning_rate=config['gen_learning_rate'],\n",
    "                            beta_1=config['gen_adam_beta1']),\n",
    "            loss_fn=BinaryCrossentropy(from_logits=False)\n",
    "        )\n",
    "        return cgan\n",
    "\n",
    "    def get_current_config(self, hp):\n",
    "        return {key: hp.get(key) for key in self.last_build_config_keys}       \n",
    "\n",
    "    def fit(self, hp, model, x, y, wandb_config, *args, **kwargs):\n",
    "        config = self.get_current_config(hp)\n",
    "        with wandb.init(project=PROJECT, config=config, mode=wandb_config['mode'],\n",
    "                        group=wandb_config['group']):\n",
    "            wandbcb = wandb.keras.WandbCallback(\n",
    "                monitor=\"val_within_class_inception_score\", mode=\"max\",\n",
    "                save_model=False, log_batch_frequency=50)\n",
    "            sampler = SamplerCallback(sample_freq=100)\n",
    "            garbage_collector = GarbageCollectorCallback()\n",
    "            # adding the callbacks\n",
    "            kwargs['callbacks'].extend([sampler, garbage_collector, wandbcb])\n",
    "            \n",
    "            ret = model.fit(x, y, *args,\n",
    "                            batch_size=config['batch_size'],\n",
    "                            **kwargs)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = None if USE_WANDB else 'disabled'\n",
    "group_name = 'hyperparam_optimization_hyperband_12_09_v3'\n",
    "wandb_config = {'mode': 'disabled',\n",
    "                'group': group_name}\n",
    "\n",
    "# tuner = keras_tuner.RandomSearch(\n",
    "#     CGANHypermodel(),\n",
    "#     objective=keras_tuner.Objective('val_within_class_inception_score', 'max'),\n",
    "#     max_trials=20,\n",
    "#     overwrite=True,\n",
    "#     directory='hyperparam_tuning',\n",
    "#     project_name=PROJECT,\n",
    "# )\n",
    "\n",
    "tuner = keras_tuner.Hyperband(\n",
    "    CGANHypermodel(),\n",
    "    objective=keras_tuner.Objective('val_within_class_inception_score', 'max'),\n",
    "    factor=6,\n",
    "    max_epochs=20,\n",
    "    directory='hyperparam_tuning_hyperband_12_09_v3',\n",
    "    project_name=PROJECT,\n",
    ")\n",
    "\n",
    "tuner.search(x_train[:128], labels_train[:128],\n",
    "            wandb_config,\n",
    "            epochs=8,\n",
    "            validation_data=[x_test[0]],\n",
    "            )\n",
    "\n",
    "print(tuner.get_best_hyperparameters()[0].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wg4INiTPXUk9",
    "outputId": "116cd08c-94b5-4ce1-f76c-7b988c0960b8"
   },
   "outputs": [],
   "source": [
    "config = {'latent_dim': 256,\n",
    "          'gen_label_embedding': 50,\n",
    "          'gen_label_hidden': 50,\n",
    "          'gen_input_hidden': 50,\n",
    "          'gen_conv1_channels': 128,\n",
    "          'gen_conv2_channels': 128,\n",
    "          'disc_label_embedding': 50,\n",
    "          'disc_conv1_channels': 32,\n",
    "          'disc_conv2_channels': 64,\n",
    "          'disc_dense_hidden': 128,\n",
    "          'disc_use_batchnorm': True,\n",
    "          'n_classes': N_CLASSES,\n",
    "          'epochs': 8,\n",
    "          'batch_size': 32,\n",
    "          'n_val': VAL_METRIC_N_SAMPLE,\n",
    "          'VAL_MODEL_PATH': VAL_MODEL_PATH,\n",
    "          'gen_learning_rate': 0.0002,\n",
    "          'gen_adam_beta1': 0.5,\n",
    "          'discr_learning_rate': 0.0002,\n",
    "          'discr_adam_beta1': 0.5}\n",
    "mode = None if USE_WANDB else 'disabled'\n",
    "group_name = 'hyperparam_optimization_12_09'\n",
    "with wandb.init(project=PROJECT, config=config, mode=mode, group='tmp'):\n",
    "    cgan = CGAN(config)\n",
    "\n",
    "    cgan.compile(\n",
    "        d_optimizer=Adam(learning_rate=config['discr_learning_rate'],\n",
    "                         beta_1=config['discr_adam_beta1']),\n",
    "        g_optimizer=Adam(learning_rate=config['gen_learning_rate'],\n",
    "                         beta_1=config['gen_adam_beta1']),\n",
    "        loss_fn=BinaryCrossentropy(from_logits=False)\n",
    "    )\n",
    "\n",
    "    wandbcb = wandb.keras.WandbCallback(monitor=\"val_within_class_inception_score\",\n",
    "                                        mode=\"max\", save_model=False, log_batch_frequency=50)\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        'models/{epoch:002d}/', verbose=0, save_best_only=False,\n",
    "        save_weights_only=False, mode='auto', save_freq=100)\n",
    "    sampler = SamplerCallback(sample_freq=100)\n",
    "    garbage_collector = GarbageCollectorCallback()\n",
    "\n",
    "    cgan.fit(\n",
    "        x_train[:256], labels_train[:256],\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        validation_data=[x_test[0]], #dummy validation data\n",
    "        callbacks=[\n",
    "            checkpointer,\n",
    "            sampler,\n",
    "            garbage_collector,\n",
    "            wandbcb\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4ZHXnriqKY-"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "-qgb0HIcqRb3",
    "outputId": "1e5a0bf9-3c29-4948-979d-39e3e79280ab"
   },
   "outputs": [],
   "source": [
    "e = 1\n",
    "cgan = CGAN(latent_dim=100, n_classes=10)\n",
    "cgan.load_models(f'models/{e:02d}/generator.h5', f'models/{e:02d}/discriminator.h5')\n",
    "cgan.sample_images(method='show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "2ChrUTFTt4w3",
    "outputId": "11a9e81d-cbb7-4feb-84fd-c63121093cda"
   },
   "outputs": [],
   "source": [
    "e = 3\n",
    "cgan = CGAN(latent_dim=100, n_classes=10)\n",
    "cgan.load_models(f'models/{e:02d}/generator.h5', f'models/{e:02d}/discriminator.h5')\n",
    "cgan.sample_images(method='show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "_Flbu0EXt45-",
    "outputId": "36637b97-5712-4224-ac6b-df72f716edb0"
   },
   "outputs": [],
   "source": [
    "e = 5\n",
    "cgan = CGAN(latent_dim=100, n_classes=10)\n",
    "cgan.load_models(f'models/{e:02d}/generator.h5', f'models/{e:02d}/discriminator.h5')\n",
    "cgan.sample_images(method='show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "tEy36lCwt49D",
    "outputId": "ddcb7090-74e5-43da-d145-1610f67d6abd"
   },
   "outputs": [],
   "source": [
    "e = 7\n",
    "cgan = CGAN(latent_dim=100, n_classes=10)\n",
    "cgan.load_models(f'models/{e:02d}/generator.h5', f'models/{e:02d}/discriminator.h5')\n",
    "cgan.sample_images(method='show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "Df5J2Dyot5AR",
    "outputId": "491fa408-7979-4aae-8828-b505b3d1b556"
   },
   "outputs": [],
   "source": [
    "e = 9\n",
    "cgan = CGAN(latent_dim=100, n_classes=10)\n",
    "cgan.load_models(f'models/{e:02d}/generator.h5', f'models/{e:02d}/discriminator.h5')\n",
    "cgan.sample_images(method='show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "id": "BMMgM96Lt5DB",
    "outputId": "2db79113-d2f1-4f94-df21-e912c2c8640c"
   },
   "outputs": [],
   "source": [
    "e = 10\n",
    "cgan = CGAN(latent_dim=100, n_classes=10)\n",
    "cgan.load_models(f'models/{e:02d}/generator.h5', f'models/{e:02d}/discriminator.h5')\n",
    "cgan.sample_images(method='show')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSbcTDPnjFur"
   },
   "source": [
    "# Download models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBIUzGa9S0sX",
    "outputId": "e8d1e1a0-a49d-4d23-9abf-6eb3843f9973"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/models.zip /content/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "dLeX1tpJjOcB",
    "outputId": "80a3fcbf-2ce6-4bfe-defd-cec033354f1c"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/models.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPTYIu7e4QKlZWi2s3+mk7v",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
